{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL assignmnet 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJksmImTx4eVAoSZ18FAUN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lezya/Deep-Learning/blob/master/RL_assignmnet_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T29hEuYy9GPc",
        "colab_type": "code",
        "outputId": "fba0755c-5e14-4db4-e693-e524f485271c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Init environment\n",
        "# Lets use a smaller 3x3 custom map for faster computations\n",
        "custom_map3x3 = [\n",
        "    'SFF',\n",
        "    'FFF',\n",
        "    'FHG',\n",
        "]\n",
        "env = gym.make(\"FrozenLake-v0\", desc=custom_map3x3)\n",
        "# TODO: Uncomment the following line to try the default map (4x4):\n",
        "#env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "# Uncomment the following lines for even larger maps:\n",
        "#random_map = generate_random_map(size=5, p=0.8)\n",
        "#env = gym.make(\"FrozenLake-v0\", desc=random_map)\n",
        "\n",
        "# Init some useful variables:\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "r = np.zeros(n_states) # the r vector is zero everywhere except for the goal state (last state)\n",
        "r[-1] = 1.\n",
        "\n",
        "gamma = 0.8\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns the transition probability matrix P for a policy \"\"\"\n",
        "def trans_matrix_for_policy(policy):\n",
        "    transitions = np.zeros((n_states, n_states))\n",
        "    for s in range(n_states):\n",
        "        probs = env.P[s][policy[s]]\n",
        "        for el in probs:\n",
        "            transitions[s, el[1]] += el[0]\n",
        "    return transitions\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns terminal states \"\"\"\n",
        "def terminals():\n",
        "    terms = []\n",
        "    for s in range(n_states):\n",
        "        # terminal is when we end with probability 1 in terminal:\n",
        "        if env.P[s][0][0][0] == 1.0 and env.P[s][0][0][3] == True:\n",
        "            terms.append(s)\n",
        "    return terms\n",
        "\n",
        "\n",
        "def value_policy(policy):\n",
        "    #P = trans_matrix_for_policy(policy)\n",
        "    P=env.env.P\n",
        "    print(P)\n",
        "    maxiter = 100\n",
        "    values=np.zeros(env.observation_space.n)\n",
        "    # TODO: calculate and return v\n",
        "    for i in range(maxiter):\n",
        "\t        prev_v = np.copy(values)\n",
        "\t        for state in range(n_states):\n",
        "\t            Q_value = []\n",
        "\t            for action in range(n_actions):\n",
        "\t                next_states_rewards = []\n",
        "\t                for trans_prob, next_state, reward_prob, _ in P[state][action]:\n",
        "\t                    next_states_rewards.append((trans_prob * (reward_prob + gamma * prev_v[next_state])))\n",
        "\t                Q_value.append(sum(next_states_rewards))                 \n",
        "\t            values[state] = max(Q_value)\n",
        "                \n",
        "    # (P, r and gamma already given)\n",
        "    print(\"praveen\",reward_prob)\n",
        "    return values\n",
        "\n",
        "\n",
        "def bruteforce_policies():\n",
        "    terms = terminals()\n",
        "    optimalpolicies = []\n",
        "\n",
        "    policy = np.zeros(n_states, dtype=np.int)  # in the discrete case a policy is just an array with action = policy[state]\n",
        "    optimalvalue = np.zeros(n_states)\n",
        "    \n",
        "    # TODO: implement code that tries all possible policies, calculate the values using def value_policy. \n",
        "    #Find the optimal values and the optimal policies to answer the exercise questions.\n",
        "\n",
        "    print (\"Optimal value function:\")\n",
        "    print(optimalvalue)\n",
        "    print (\"number optimal policies:\")\n",
        "    print (len(optimalpolicies))\n",
        "    print (\"optimal policies:\")\n",
        "    print (np.array(optimalpolicies))\n",
        "    return optimalpolicies\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # print the environment\n",
        "    print(\"current environment: \")\n",
        "    env.render()\n",
        "    print(\"\")\n",
        "\n",
        "    # Here a policy is just an array with the action for a state as element\n",
        "    policy_left = np.zeros(n_states, dtype=np.int)  # 0 for all states\n",
        "    policy_right = np.ones(n_states, dtype=np.int) * 2  # 2 for all states\n",
        "\n",
        "    # Value functions:\n",
        "    print(\"Value function for policy_left (always going left):\")\n",
        "    print (value_policy(policy_left))\n",
        "    print(\"Value function for policy_right (always going right):\")\n",
        "    print (value_policy(policy_right))\n",
        "\n",
        "    optimalpolicies = bruteforce_policies()\n",
        "\n",
        "\n",
        "    # This code can be used to \"rollout\" a policy in the environment:\n",
        "    '''\n",
        "    print (\"rollout policy:\")\n",
        "    maxiter = 100\n",
        "    state = env.reset()\n",
        "    for i in range(maxiter):\n",
        "        new_state, reward, done, info = env.step(optimalpolicies[0][state])\n",
        "        env.render()\n",
        "        state=new_state\n",
        "        if done:\n",
        "            print (\"Finished episode\")\n",
        "            break'''\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current environment: \n",
            "\n",
            "\u001b[41mS\u001b[0mFF\n",
            "FFF\n",
            "FHG\n",
            "\n",
            "Value function for policy_left (always going left):\n",
            "{0: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 2: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 1: {0: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 2: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 3: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 2: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 5, 0.0, False)], 1: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 2: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]}, 3: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False)], 1: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 2: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False)]}, 4: {0: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 1: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 5, 0.0, False)], 2: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 3: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 3, 0.0, False)]}, 5: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 1.0, True)], 1: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 1.0, True), (0.3333333333333333, 5, 0.0, False)], 2: [(0.3333333333333333, 8, 1.0, True), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 4, 0.0, False)]}, 6: {0: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 6, 0.0, False)], 1: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(1.0, 8, 0, True)], 1: [(1.0, 8, 0, True)], 2: [(1.0, 8, 0, True)], 3: [(1.0, 8, 0, True)]}}\n",
            "praveen 0\n",
            "[0.12439178 0.20803453 0.32786885 0.13404287 0.2442261  0.57377049\n",
            " 0.07659592 0.         0.        ]\n",
            "Value function for policy_right (always going right):\n",
            "{0: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 2: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 1: {0: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 2: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 3: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 2: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 5, 0.0, False)], 1: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 2: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]}, 3: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False)], 1: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 2: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 3, 0.0, False)]}, 4: {0: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 1: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 5, 0.0, False)], 2: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 3: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 3, 0.0, False)]}, 5: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 1.0, True)], 1: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 1.0, True), (0.3333333333333333, 5, 0.0, False)], 2: [(0.3333333333333333, 8, 1.0, True), (0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 5, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 4, 0.0, False)]}, 6: {0: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 6, 0.0, False)], 1: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 6, 0.0, False)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(1.0, 8, 0, True)], 1: [(1.0, 8, 0, True)], 2: [(1.0, 8, 0, True)], 3: [(1.0, 8, 0, True)]}}\n",
            "praveen 0\n",
            "[0.12439178 0.20803453 0.32786885 0.13404287 0.2442261  0.57377049\n",
            " 0.07659592 0.         0.        ]\n",
            "Optimal value function:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "number optimal policies:\n",
            "0\n",
            "optimal policies:\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md64xq7Amvye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Init environment\n",
        "# Lets use a smaller 3x3 custom map for faster computations\n",
        "custom_map3x3 = [\n",
        "    'SFF',\n",
        "    'FFF',\n",
        "    'FHG',\n",
        "]\n",
        "env = gym.make(\"FrozenLake-v0\", desc=custom_map3x3)\n",
        "\n",
        "# TODO: Uncomment the following line to try the default map (4x4):\n",
        "#env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "# Uncomment the following lines for even larger maps:\n",
        "#random_map = generate_random_map(size=5, p=0.8)\n",
        "#env = gym.make(\"FrozenLake-v0\", desc=random_map)\n",
        "\n",
        "# Init some useful variables:\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "print(\"n_states=\",n_states)\n",
        "print(\"And n_actions=\", n_actions)\n",
        "r = np.zeros(n_states) # the r vector is zero everywhere except for the goal state (last state)\n",
        "r[-1] = 1.\n",
        "\n",
        "\n",
        "gamma = 0.8\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns the transition probability matrix P for a policy \"\"\"\n",
        "def trans_matrix_for_policy(policy):\n",
        "    transitions = np.zeros((n_states, n_states))\n",
        "    #print(transitions)\n",
        "    for s in range(n_states):\n",
        "        probs = env.P[s][policy[s]]\n",
        "        for el in probs:\n",
        "            transitions[s, el[1]] += el[0]\n",
        "            print(\"EL\",el[1])\n",
        "    print (\"Srijay\",transitions)\n",
        "    #print (env.env.P)\n",
        "    return transitions\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns terminal states \"\"\"\n",
        "def terminals():\n",
        "    terms = []\n",
        "    for s in range(n_states):\n",
        "        # terminal is when we end with probability 1 in terminal:\n",
        "        if env.P[s][0][0][0] == 1.0 and env.P[s][0][0][3] == True:\n",
        "            terms.append(s)\n",
        "    #print(\"Terms\")\n",
        "    #print(terms)\n",
        "    return terms\n",
        "\n",
        "\n",
        "def value_policy(policy):\n",
        "    P = trans_matrix_for_policy(policy)\n",
        "    # TODO: calculate and return v\n",
        "    # (P, r and gamma already given)\n",
        "    return None\n",
        "\n",
        "\n",
        "def bruteforce_policies():\n",
        "    terms = terminals()\n",
        "    optimalpolicies = []\n",
        "\n",
        "    policy = np.zeros(n_states, dtype=np.int)  # in the discrete case a policy is just an array with action = policy[state]\n",
        "    optimalvalue = np.zeros(n_states)\n",
        "    \n",
        "    # TODO: implement code that tries all possible policies, calculate the values using def value_policy. \n",
        "    #Find the optimal values and the optimal policies to answer the exercise questions.\n",
        "\n",
        "    print (\"Optimal value function:\")\n",
        "    print(optimalvalue)\n",
        "    print (\"number optimal policies:\")\n",
        "    print (len(optimalpolicies))\n",
        "    print (\"optimal policies:\")\n",
        "    print (np.array(optimalpolicies))\n",
        "    return optimalpolicies\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # print the environment\n",
        "    print(\"current environment: \")\n",
        "    env.render()\n",
        "    print(\"\")\n",
        "\n",
        "    # Here a policy is just an array with the action for a state as element\n",
        "    policy_left = np.zeros(n_states, dtype=np.int)  # 0 for all states\n",
        "    print(policy_left)\n",
        "    policy_right = np.ones(n_states, dtype=np.int) * 1  # 2 for all states\n",
        "    #print(policy_right)\n",
        "\n",
        "    # Value functions:\n",
        "    print(\"Value function for policy_left (always going left):\")\n",
        "    print (value_policy(policy_left))\n",
        "    print(\"Value function for policy_right (always going right):\")\n",
        "    print (value_policy(policy_right))\n",
        "\n",
        "    optimalpolicies = bruteforce_policies()\n",
        "\n",
        "\n",
        "    # This code can be used to \"rollout\" a policy in the environment:\n",
        "    \n",
        "    '''print (\"rollout policy:\")\n",
        "    maxiter = 100\n",
        "    state = env.reset()\n",
        "    for i in range(maxiter):\n",
        "        new_state, reward, done, info = env.step(optimalpolicies[0][state])\n",
        "        env.render()\n",
        "        state=new_state\n",
        "        if done:\n",
        "            print (\"Finished episode\")\n",
        "            break'''\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl-dx4rDxYlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "9881bb08-a18a-4c27-f6e7-1114e7bf4fae"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Init environment\n",
        "# Lets use a smaller 3x3 custom map for faster computations\n",
        "'''custom_map3x3 = [\n",
        "    'SFF',\n",
        "    'FFF',\n",
        "    'FHG',\n",
        "]'''\n",
        "#env = gym.make(\"FrozenLake-v0\", desc=custom_map3x3)\n",
        "# TODO: Uncomment the following line to try the default map (4x4):\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "# Uncomment the following lines for even larger maps:\n",
        "#random_map = generate_random_map(size=5, p=0.8)\n",
        "#env = gym.make(\"FrozenLake-v0\", desc=random_map)\n",
        "\n",
        "# Init some useful variables:\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "r = np.zeros(n_states) # the r vector is zero everywhere except for the goal state (last state)\n",
        "r[-1] = 1.\n",
        "\n",
        "gamma = 0.8\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns the transition probability matrix P for a policy \"\"\"\n",
        "def trans_matrix_for_policy(policy):\n",
        "    transitions = np.zeros((n_states, n_states))\n",
        "    for s in range(n_states):\n",
        "        probs = env.P[s][policy[s]]\n",
        "        for el in probs:\n",
        "            transitions[s, el[1]] += el[0]\n",
        "    return transitions\n",
        "\n",
        "\n",
        "\"\"\" This is a helper function that returns terminal states \"\"\"\n",
        "def terminals():\n",
        "    terms = []\n",
        "    for s in range(n_states):\n",
        "        # terminal is when we end with probability 1 in terminal:\n",
        "        if env.P[s][0][0][0] == 1.0 and env.P[s][0][0][3] == True:\n",
        "            terms.append(s)\n",
        "    return terms\n",
        "\n",
        "\n",
        "def value_policy(policy):\n",
        "    P = trans_matrix_for_policy(policy)\n",
        "    # TODO: calculate and return v\n",
        "    # (P, r and gamma already given)\n",
        "    I = np.ones((n_states,n_states), dtype=np.int)\n",
        "    inv = np.linalg.inv(I - (gamma*P))\n",
        "    #print(\"INV:\")\n",
        "    #print(inv)\n",
        "    r_tr = np.transpose(r)\n",
        "    #vector = np.zeros((n_states))\n",
        "    v = np.dot(inv,r_tr)\n",
        "    return v\n",
        "\n",
        "\n",
        "def bruteforce_policies():\n",
        "    terms = terminals()\n",
        "    optimalpolicies = []\n",
        "\n",
        "    policy = np.zeros(n_states, dtype=np.int)  # in the discrete case a policy is just an array with action = policy[state]\n",
        "    optimalvalue = np.zeros(n_states)\n",
        "    \n",
        "    # TODO: implement code that tries all possible policies, calculate the values using def value_policy. Find the optimal values and the optimal policies to answer the exercise questions.\n",
        "    values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    max_iterations = 1000\n",
        "    state_prob = env.env.P\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "\t        prev_v = np.copy(values)\n",
        "\t        for state in range(n_states):\n",
        "\t            Q_value = []\n",
        "\t            for action in range(n_actions):\n",
        "\t                next_states_rewards = []\n",
        "\t                for trans_prob, next_state, reward_prob, _ in state_prob[state][action]: \n",
        "\t                   \n",
        "\t                    next_states_rewards.append((trans_prob * (reward_prob + gamma * prev_v[next_state]))) \n",
        "\t                \n",
        "\t                Q_value.append(sum(next_states_rewards))\n",
        "\n",
        "\t            \n",
        "\t            optimalvalue[state] = max(Q_value)\n",
        "\n",
        "    for s in range(n_states):\n",
        "\t        q_sa = np.zeros(n_actions)\n",
        "\t        for a in range(n_actions):\n",
        "\t            for next_sr in state_prob[s][a]:\n",
        "\t                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "\t                p, s_, r, _ = next_sr\n",
        "\t                q_sa[a] += (p * (r + gamma * optimalvalue[s_]))\n",
        "\t        optimalpolicies.append(np.argmax(q_sa)) \n",
        "      \n",
        "    \n",
        "\n",
        "    print (\"Optimal value function:\")\n",
        "    print(optimalvalue)\n",
        "    print (\"number optimal policies:\")\n",
        "    print (len(optimalpolicies))\n",
        "    print (\"optimal policies:\")\n",
        "    print (np.array(optimalpolicies))\n",
        "    return optimalpolicies\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # print the environment\n",
        "    print(\"current environment: \")\n",
        "    env.render()\n",
        "    print(\"\")\n",
        "\n",
        "    # Here a policy is just an array with the action for a state as element\n",
        "    policy_left = np.zeros(n_states, dtype=np.int)  # 0 for all states\n",
        "    policy_right = np.ones(n_states, dtype=np.int) * 2  # 2 for all states\n",
        "\n",
        "    # Value functions:\n",
        "    print(\"Value function for policy_left (always going left):\")\n",
        "    print (value_policy(policy_left))\n",
        "    print(\"Value function for policy_right (always going right):\")\n",
        "    print (value_policy(policy_right))\n",
        "\n",
        "    optimalpolicies = bruteforce_policies()\n",
        "\n",
        "\n",
        "    # This code can be used to \"rollout\" a policy in the environment:\n",
        "    \"\"\"\n",
        "    print (\"rollout policy:\")\n",
        "    maxiter = 100\n",
        "    state = env.reset()\n",
        "    for i in range(maxiter):\n",
        "        new_state, reward, done, info = env.step(optimalpolicies[0][state])\n",
        "        env.render()\n",
        "        state=new_state\n",
        "        if done:\n",
        "            print (\"Finished episode\")\n",
        "            break\"\"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current environment: \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Value function for policy_left (always going left):\n",
            "[  1.25   1.25  19.   -16.5    1.25   1.25 -16.5    1.25   1.25   1.25\n",
            " -16.5    1.25   1.25   1.25  19.     0.  ]\n",
            "Value function for policy_right (always going right):\n",
            "[-1.80143985e+15 -1.80143985e+15  1.80143985e+15  2.35516496e-01\n",
            "  3.60287970e+15 -7.40391523e-01 -1.80143985e+15 -4.58598469e-01\n",
            "  1.80143985e+15 -3.60287970e+15 -1.80143985e+15  1.00719572e+00\n",
            "  7.61718750e-01  1.80143985e+15  1.80143985e+15 -2.10197368e+00]\n",
            "Optimal value function:\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.33333333 0.        ]\n",
            "number optimal policies:\n",
            "16\n",
            "optimal policies:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}